# UI Spec

> The user interface for the AI observability platform. Two entry points — a developer-facing view and a cross-functional review queue — share a single application. The UI consumes the HTTP API defined in `specs/06-api.md`.

Cross-cutting rules are in `CONSTITUTION.md`. Object schemas are in `DATA-MODEL.md`.

---

## Overview

The UI serves two distinct audiences with different mental models:

- **Developers** need dense, navigable views of traces, experiments, and datasets. They are comfortable with technical data (JSON, token counts, span trees) and need to act quickly on quality signals.
- **Reviewers** (PMs, domain experts, QA, subject-matter experts) need a simplified interface that surfaces the right question — "was this response good?" — without requiring knowledge of the underlying platform.

Both audiences share one application. Entry point is determined by which section of the sidebar they navigate to.

---

## Application Shell

### Project Selector

All data in the platform is project-scoped. The shell displays the active project name at the top of the sidebar. A project switcher allows switching between projects without a full page reload.

- On first load, if only one project exists, it is selected automatically.
- If multiple projects exist, the user is prompted to select one before any content loads.
- The active `project_id` is reflected in the URL (e.g., `/projects/proj-1/traces`).

### Sidebar Navigation

Two sections, visually separated:

**Developer section:**
- Dashboard
- Traces
- Experiments
- Datasets

**Reviewer section:**
- Review Queue

---

## Developer View

### 1. Dashboard

The landing page for the developer view. Provides a health snapshot of the project.

**Must display:**

| Section | Content |
|---|---|
| Traces (last 24h) | Total trace count, error rate (% of traces with at least one errored span), average root span duration |
| Traces (last 7d) | Same three metrics for the wider window |
| Experiments | Count of experiments by status (`created`, `running`, `completed`); most recent completed experiment name + mean score per scorer |
| Review Queue | Count of traces currently in the queue with status `pending` |
| Datasets | Name, item count, and version for each dataset in the project, ordered by `updated_at` descending |

**Data sources:**
- `GET /v1/projects/:id/summary` (see `specs/06-api.md` additions)
- `GET /v1/datasets?project_id=` for dataset list

**No charts in v1.** All data is displayed as numbers and tables. Chart visualizations are deferred.

---

### 2. Traces

#### Trace List

A paginated table of all traces for the project, ordered by `created_at` descending.

**Columns:**

| Column | Source | Notes |
|---|---|---|
| Root span name | `root_span.name` | Truncated to 60 chars |
| Span count | `span_count` | |
| Duration | `root_span.duration_ms` | Displayed as `1.2s`, `340ms`, etc. |
| Tokens | `sum of tokens_input + tokens_output` across all spans | Displayed as `1,234 tok` |
| Model | `root_span.model` or most common model across spans | |
| Errors | Boolean indicator | Red dot if any span has `error` set |
| Queued | Boolean indicator | Checkmark if trace is in the review queue |
| Created | `created_at` | Relative time (e.g., "3 hours ago") |

**Filters (above the table):**
- Time range: last 1h / 6h / 24h / 7d / custom
- Has errors: toggle
- In queue: toggle
- Metadata key/value: free text filter applied to `trace.metadata`

**Actions per row:**
- Click row → navigate to Trace Detail
- "Add to Queue" button (inline) → `POST /v1/traces/:id/queue`

#### Trace Detail

Shows the full execution tree for a single trace.

A developer can view all spans in a trace as a navigable tree and inspect any individual span's full details. The tree reflects the parent-child hierarchy of spans. Spans can be expanded and collapsed. Error spans are visually distinguished from successful ones.

When a developer selects a span, they can see:
- Span name and status (ok / error)
- Start time, end time, duration
- Model, tokens_input, tokens_output (if present)
- Input and output, rendered as formatted JSON with a toggle to plain text
- Error details — message, type, and stack trace (if present)
- Metadata as key-value pairs
- All scores attached to this span
- All annotations scoped to this span

A developer can also see all annotations on the trace as a whole (not just span-scoped ones). For each annotation, they can convert it into a dataset item by selecting a target dataset — this calls `POST /v1/annotations/:id/to-dataset-item`.

**Trace-level actions:**
- "Add to Queue" — `POST /v1/traces/:id/queue` — not available if the trace is already queued
- "Delete Trace" — `DELETE /v1/traces/:id` — requires confirmation before executing

---

### 3. Experiments

#### Experiment List

A paginated table of all experiments for the project, ordered by `created_at` descending.

**Columns:**

| Column | Source |
|---|---|
| Name | `experiment.name` |
| Dataset | `dataset.name` (linked) |
| Status | Badge: `created` / `running` / `completed` |
| Runs | `run_count` / `dataset_item_count` (e.g., "42 / 50") |
| Mean score | Per scorer, from summary — one column per distinct `scorer_name` across all experiments |
| Created | `created_at` (relative) |

**Actions:**
- Click row → Experiment Detail
- "Compare" — checkbox selection of exactly two experiments → navigate to Comparison view

#### Experiment Detail

Shows per-run results for a single experiment.

**Header:**
- Experiment name, dataset name (linked), status badge, created date
- If status is `running`: "Mark Complete" button → `POST /v1/experiments/:id/complete`
- Aggregate scores from summary: one stat block per scorer (mean, min, max)
- Threshold result badge if threshold was evaluated: green PASS / red FAIL with gap value

**Runs table:**

| Column | Source |
|---|---|
| Input | `dataset_item.input` rendered as plain text (truncated to 120 chars) |
| Output | `experiment_run.output` rendered as plain text (truncated to 120 chars) |
| Trace | Link icon → Trace Detail (if `trace_id` present) |
| Score per scorer | One column per scorer; numeric shown as `0.85`, categorical as label badge |

- Clicking a run row expands it inline to show full input, full output, all scores with rationale, and a link to the associated trace.
- Sortable by any score column.
- Filterable by score range (e.g., "show only runs where exact_match < 0.5").

#### Experiment Comparison

A view of two experiments on the same dataset that allows a developer to quantify the impact of a change.

A developer can see aggregate quality metrics for both experiments side by side: base mean, compare mean, and delta per scorer, along with counts of improved, regressed, and unchanged items. Positive deltas are visually distinguished from negative ones so regressions are immediately apparent.

A developer can also see per-item results: for each dataset item, what score the base experiment produced and what score the candidate experiment produced, with the delta between them. Items where the candidate regressed are visually distinguished from those that improved.

The developer can swap which experiment is treated as the baseline.

---

### 4. Datasets

#### Dataset List

A table of all datasets for the project.

**Columns:** Name, Item count, Version, Created, Updated.

**Actions:**
- Click row → Dataset Detail
- "New Dataset" action — developer provides a name and optional description → `POST /v1/datasets`

#### Dataset Detail

Shows metadata and a paginated table of items.

**Header:** Dataset name, description, version badge, item count, created/updated dates.

**Items table:**

| Column | Source |
|---|---|
| Input | `item.input` rendered as plain text (truncated to 120 chars) |
| Expected output | `item.expected_output` rendered as plain text; "—" if absent |
| Source | `item.metadata.source` if present (e.g., "manual", "annotation", "import") |
| Trace | Link icon if `metadata.source_trace_id` present |
| Created | `created_at` (relative) |

**Actions:**
- "Add Item" action — developer provides an input and optional expected output → `POST /v1/datasets/:id/items`
- "Import JSONL" button → file picker → `POST /v1/datasets/:id/items/import` — shows import result summary (imported, skipped) after completion

---

## Review Queue

The entry point for non-technical reviewers. Deliberately simpler than the developer view. No trace trees, no JSON by default, no experiment tables.

### Queue List

A feed of traces pending review, ordered by `queued_at` ascending (oldest first — work through the queue in order).

**Each queue item card shows:**
- A "Question" — the root span's `input`, rendered as plain conversational text. If `input` is a JSON object with a `messages` array (chat format), the last user message is extracted and shown. Otherwise, the raw string value is shown.
- An "Answer" — the root span's `output`, rendered as plain text.
- Status badge: `pending` / `annotated`
- Annotator count: how many annotations this trace already has (e.g., "2 reviews")
- Time in queue: relative time since queued

**Filters:**
- Status: All / Pending / Annotated
- Annotated by me: toggle (filters to traces where `annotator` matches the current user's identifier)

**Actions per card:**
- Click → open Annotation Panel
- "Skip" → moves item to end of queue (client-side only; does not call the API)

---

### Annotation Panel

The annotation panel is opened when a reviewer selects a queue item. It provides enough space to display the full input and output of a trace without truncation.

A reviewer can see:
- The trace's input, rendered as readable text using the same extraction logic as the queue list. Inputs longer than 500 characters can be expanded to show the full text.
- The trace's output, rendered as readable text.
- A link to the full Trace Detail view for developers who want deeper context. Following this link does not interrupt the annotation flow.
- All previous annotations on this trace, visible as read-only. If there are more than two, they are summarised and expandable.

A reviewer can submit an annotation with any combination of:
- **Label** — a categorical classification for the response. Common labels are offered as suggestions (`correct`, `hallucination`, `off-topic`, `incomplete`, `harmful`, `other`); the reviewer can also provide a custom label.
- **Correction** — the response the application should have given. The correction field is pre-filled with the current output so the reviewer can edit rather than retype.
- **Notes** — free-text observations.

At least one of label, correction, or notes must be provided before submission is allowed. The annotator identifier is populated with the current user's identifier by default and can be changed before submitting.

On submission (`POST /v1/annotations`), the reviewer is offered two next actions:
1. **Save to Dataset** — convert the annotation to a dataset item by selecting a target dataset (`POST /v1/annotations/:id/to-dataset-item`).
2. **Next** — move to the next pending item in the queue without saving to a dataset.

If a reviewer leaves the annotation panel without submitting, no annotation is saved. There is no draft state in v1.

---

## Rendering Contract

The UI must apply these rules consistently when displaying `input` or `output` values from spans or dataset items:

| Value type | How to render |
|---|---|
| String | Display as-is |
| Object with `messages` array (OpenAI chat format) | Extract the last message with `role: "user"` and display its `content`. Show a "chat format" badge. |
| Object (other) | Display as formatted, syntax-highlighted JSON. Offer "Plain text" toggle. |
| Array | Display as formatted JSON. Offer "Plain text" toggle. |
| `null` or absent | Display as "—" (em dash) |
| Number / boolean | Display as string representation |

In the **Review Queue and Annotation Panel**, the "Plain text" toggle is hidden and JSON is never shown by default. Reviewer always sees extracted readable text.

In the **Developer View** (trace detail, experiment runs), JSON rendering is the default with a toggle to plain text.

---

## Acceptance Criteria

```
GIVEN a project with 3 traces in the last 24 hours, 1 of which has an errored span
WHEN the Dashboard is loaded
THEN the Traces section shows count=3 and error_rate=33%

GIVEN a trace with 4 spans arranged as a tree (root → A → B, root → C)
WHEN the Trace Detail page is loaded
THEN the span tree shows root at top level, A and C as children, B as child of A
AND all spans are expanded by default

GIVEN a span whose output is a JSON object
WHEN the span is selected in Trace Detail
THEN the output is displayed as formatted, syntax-highlighted JSON
AND a "Plain text" toggle is visible

GIVEN a trace not currently in the review queue
WHEN "Add to Queue" is clicked on the Trace Detail page
THEN POST /v1/traces/:id/queue is called
AND the button changes to a disabled "In Queue" state

GIVEN two experiments on the same dataset where experiment B has a higher mean score
WHEN the Comparison view is loaded with A as base and B as compare
THEN the delta is shown in green (positive)
AND rows where B scored lower than A are highlighted red in the per-item table

GIVEN a dataset with 20 items
WHEN the Dataset Detail page is loaded
THEN all items are paginated with 50 per page by default
AND each item's input is truncated to 120 characters with a "Show more" affordance

GIVEN a queue item whose root span input is a JSON object with a messages array
  containing [{"role": "system", "content": "..."}, {"role": "user", "content": "What is the capital of France?"}]
WHEN the queue item is displayed in the Review Queue
THEN the displayed input is "What is the capital of France?"
AND a "chat format" badge is shown

GIVEN a reviewer opens the Annotation Panel and fills in only the Correction field
WHEN Submit is clicked
THEN POST /v1/annotations is called with correction set and label and notes absent
AND the submit succeeds

GIVEN a reviewer opens the Annotation Panel and fills in no fields
WHEN Submit is clicked
THEN the submission is blocked client-side
AND an inline message says "Please provide at least a label, correction, or notes"

GIVEN a successful annotation submission
WHEN the reviewer clicks "Save to Dataset" and selects a dataset
THEN POST /v1/annotations/:id/to-dataset-item is called with the selected dataset_id
AND a success message confirms the item was added

GIVEN a successful annotation submission
WHEN the reviewer clicks "Next"
THEN the panel navigates to the next pending item in the queue
AND no dataset conversion is performed

GIVEN an Experiment Detail page for an experiment with a failed threshold (actual=0.75, threshold=0.80)
WHEN the page loads
THEN a red FAIL badge is visible in the header
AND the gap is displayed as "-0.05"
```

---

## Edge Cases

| Scenario | Expected Behavior |
|---|---|
| Project has no traces | Trace List shows empty state: "No traces yet. Instrument your application to get started." with a link to docs |
| Project has no experiments | Experiment List shows empty state: "No experiments yet. Create a dataset and run your first eval." |
| Review Queue is empty | Queue List shows empty state: "Queue is empty. Add traces to the queue from the Trace Detail view." |
| Trace has no root span (partial trace) | Trace Detail shows a warning banner: "This trace has no root span. Spans may still be ingesting." Root span fields show "—". |
| Span input or output is very large (>10KB) | Display the first 2KB with a "Load full content" button. Do not truncate silently. |
| Experiment comparison requested on two experiments with different datasets | Show error state: "These experiments used different datasets and cannot be compared." with links to each experiment |
| Annotation Panel: reviewer navigates away mid-form | If any field is filled, show a browser-native "Leave page?" confirmation. If all fields are empty, navigate without prompt. |
| Dataset import returns skipped lines | Show a dismissible warning banner: "8 items imported, 2 skipped." with a "View details" toggle showing skipped line numbers and reasons |
| Trace with 0 annotations opened in Annotation Panel via queue | "Previous annotations" section is hidden entirely (not shown as empty) |
