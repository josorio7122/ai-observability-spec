# UI Spec

> The user interface for the AI observability platform. Two entry points — a developer-facing view and a cross-functional review queue — share a single application. The UI consumes the HTTP API defined in `specs/06-api.md`.

Cross-cutting rules are in `CONSTITUTION.md`. Object schemas are in `DATA-MODEL.md`.

---

## Overview

The UI serves two distinct audiences with different mental models:

- **Developers** need dense, navigable views of traces, experiments, and datasets. They are comfortable with technical data (JSON, token counts, span trees) and need to act quickly on quality signals.
- **Reviewers** (PMs, domain experts, QA, subject-matter experts) need a simplified interface that surfaces the right question — "was this response good?" — without requiring knowledge of the underlying platform.

Both audiences share one application. Entry point is determined by which section of the sidebar they navigate to.

---

## Application Shell

### Project Selector

All data in the platform is project-scoped. The shell displays the active project name at the top of the sidebar. A project switcher allows switching between projects without a full page reload.

- On first load, if only one project exists, it is selected automatically.
- If multiple projects exist, the user is prompted to select one before any content loads.
- The active `project_id` is reflected in the URL (e.g., `/projects/proj-1/traces`).

### Sidebar Navigation

Two sections, visually separated:

**Developer section:**
- Dashboard
- Traces
- Experiments
- Datasets

**Reviewer section:**
- Review Queue

---

## Developer View

### 1. Dashboard

The landing page for the developer view. Provides a health snapshot of the project.

**Must display:**

| Section | Content |
|---|---|
| Traces (last 24h) | Total trace count, error rate (% of traces with at least one errored span), average root span duration |
| Traces (last 7d) | Same three metrics for the wider window |
| Experiments | Count of experiments by status (`created`, `running`, `completed`); most recent completed experiment name + mean score per scorer |
| Review Queue | Count of traces currently in the queue with status `pending` |
| Datasets | Name, item count, and version for each dataset in the project, ordered by `updated_at` descending |

**Data sources:**
- `GET /v1/projects/:id/summary` (see `specs/06-api.md` additions)
- `GET /v1/datasets?project_id=` for dataset list

**No charts in v1.** All data is displayed as numbers and tables. Chart visualizations are deferred.

---

### 2. Traces

#### Trace List

A paginated table of all traces for the project, ordered by `created_at` descending.

**Columns:**

| Column | Source | Notes |
|---|---|---|
| Root span name | `root_span.name` | Truncated to 60 chars |
| Span count | `span_count` | |
| Duration | `root_span.duration_ms` | Displayed as `1.2s`, `340ms`, etc. |
| Tokens | `sum of tokens_input + tokens_output` across all spans | Displayed as `1,234 tok` |
| Model | `root_span.model` or most common model across spans | |
| Errors | Boolean indicator | Red dot if any span has `error` set |
| Queued | Boolean indicator | Checkmark if trace is in the review queue |
| Created | `created_at` | Relative time (e.g., "3 hours ago") |

**Filters (above the table):**
- Time range: last 1h / 6h / 24h / 7d / custom
- Has errors: toggle
- In queue: toggle
- Metadata key/value: free text filter applied to `trace.metadata`

**Actions per row:**
- Click row → navigate to Trace Detail
- "Add to Queue" button (inline) → `POST /v1/traces/:id/queue`

#### Trace Detail

Shows the full execution tree for a single trace.

**Layout:** Two-column. Left: span tree. Right: span detail panel (updates on span selection).

**Span tree (left panel):**
- Root span at top; children indented beneath their parent
- Each span row shows: name, duration, token count (if present), model (if present)
- Error spans have a red left border
- Clicking a span selects it and populates the right panel
- All spans expanded by default; collapse toggle per node

**Span detail panel (right panel):**
- Span name, status (ok / error)
- Start time, end time, duration
- Model, tokens_input, tokens_output (if present)
- Input: rendered as formatted JSON with syntax highlighting. Toggle to "Plain text" view.
- Output: same rendering as input
- Error: if present, show `error.message` and `error.type` prominently; `error.stack` in a collapsible section
- Metadata: key-value table
- Scores on this span: list of Score objects (scorer_name, value, rationale)
- Annotations on this span: list of Annotation objects (annotator, label, correction, notes)

**Trace-level actions (top of page):**
- "Add to Queue" — `POST /v1/traces/:id/queue` — disabled if already queued
- "Delete Trace" — `DELETE /v1/traces/:id` — with confirmation dialog

**Annotations panel (below span tree):**
- Shows all annotations on the trace (not just the selected span)
- Each annotation shows: annotator, label, correction (if any), notes (if any), created_at
- "Convert to Dataset Item" button per annotation — opens a modal to select target dataset, then calls `POST /v1/annotations/:id/to-dataset-item`

---

### 3. Experiments

#### Experiment List

A paginated table of all experiments for the project, ordered by `created_at` descending.

**Columns:**

| Column | Source |
|---|---|
| Name | `experiment.name` |
| Dataset | `dataset.name` (linked) |
| Status | Badge: `created` / `running` / `completed` |
| Runs | `run_count` / `dataset_item_count` (e.g., "42 / 50") |
| Mean score | Per scorer, from summary — one column per distinct `scorer_name` across all experiments |
| Created | `created_at` (relative) |

**Actions:**
- Click row → Experiment Detail
- "Compare" — checkbox selection of exactly two experiments → navigate to Comparison view

#### Experiment Detail

Shows per-run results for a single experiment.

**Header:**
- Experiment name, dataset name (linked), status badge, created date
- If status is `running`: "Mark Complete" button → `POST /v1/experiments/:id/complete`
- Aggregate scores from summary: one stat block per scorer (mean, min, max)
- Threshold result badge if threshold was evaluated: green PASS / red FAIL with gap value

**Runs table:**

| Column | Source |
|---|---|
| Input | `dataset_item.input` rendered as plain text (truncated to 120 chars) |
| Output | `experiment_run.output` rendered as plain text (truncated to 120 chars) |
| Trace | Link icon → Trace Detail (if `trace_id` present) |
| Score per scorer | One column per scorer; numeric shown as `0.85`, categorical as label badge |

- Clicking a run row expands it inline to show full input, full output, all scores with rationale, and a link to the associated trace.
- Sortable by any score column.
- Filterable by score range (e.g., "show only runs where exact_match < 0.5").

#### Experiment Comparison

Side-by-side view of two experiments on the same dataset.

**Layout:**

- Top: aggregate comparison table — one row per scorer, columns: scorer name, base mean, compare mean, delta (colored green if positive, red if negative), improved count, regressed count, unchanged count.
- Bottom: per-item table — dataset item input, base score, compare score, delta. Rows where delta is negative are highlighted red; positive delta highlighted green.
- "Base" and "Compare" experiment names shown in column headers with swap button.

---

### 4. Datasets

#### Dataset List

A table of all datasets for the project.

**Columns:** Name, Item count, Version, Created, Updated.

**Actions:**
- Click row → Dataset Detail
- "New Dataset" button → inline form (name, description) → `POST /v1/datasets`

#### Dataset Detail

Shows metadata and a paginated table of items.

**Header:** Dataset name, description, version badge, item count, created/updated dates.

**Items table:**

| Column | Source |
|---|---|
| Input | `item.input` rendered as plain text (truncated to 120 chars) |
| Expected output | `item.expected_output` rendered as plain text; "—" if absent |
| Source | `item.metadata.source` if present (e.g., "manual", "annotation", "import") |
| Trace | Link icon if `metadata.source_trace_id` present |
| Created | `created_at` (relative) |

**Actions:**
- "Add Item" button → inline form with input + expected_output fields → `POST /v1/datasets/:id/items`
- "Import JSONL" button → file picker → `POST /v1/datasets/:id/items/import` — shows import result summary (imported, skipped) after completion

---

## Review Queue

The entry point for non-technical reviewers. Deliberately simpler than the developer view. No trace trees, no JSON by default, no experiment tables.

### Queue List

A feed of traces pending review, ordered by `queued_at` ascending (oldest first — work through the queue in order).

**Each queue item card shows:**
- A "Question" — the root span's `input`, rendered as plain conversational text. If `input` is a JSON object with a `messages` array (chat format), the last user message is extracted and shown. Otherwise, the raw string value is shown.
- An "Answer" — the root span's `output`, rendered as plain text.
- Status badge: `pending` / `annotated`
- Annotator count: how many annotations this trace already has (e.g., "2 reviews")
- Time in queue: relative time since queued

**Filters:**
- Status: All / Pending / Annotated
- Annotated by me: toggle (filters to traces where `annotator` matches the current user's identifier)

**Actions per card:**
- Click → open Annotation Panel
- "Skip" → moves item to end of queue (client-side only; does not call the API)

---

### Annotation Panel

Opens as a full-page view (or large side panel) when a queue item is clicked.

**Layout:** Two sections — Review and Submit.

#### Review Section

Displays the trace context in plain language:

- **Input** — the root span's input, rendered as readable text (same extraction logic as queue list). If the input is long, show the first 500 characters with a "Show more" toggle.
- **Output** — the root span's output, rendered as readable text.
- **"Show full trace" link** — opens the Trace Detail view (developer view) in a new tab. Does not interrupt the annotation flow.
- **Previous annotations** — if the trace already has annotations, show them as read-only cards (annotator, label, correction, notes). Collapsed by default if there are more than 2.

#### Submit Section

Three fields. All optional individually, but at least one must be filled before submitting (enforced client-side before the API call):

| Field | Type | Description |
|---|---|---|
| **Label** | Dropdown + free text | Common labels pre-populated: `correct`, `hallucination`, `off-topic`, `incomplete`, `harmful`, `other`. Reviewer can type a custom label. |
| **Correction** | Text area | Pre-populated with the current output. Reviewer edits it to show what the correct response should have been. |
| **Notes** | Text area | Free-form observations. |

**Annotator field:** A text field pre-populated from the user's session (implementation-defined). Reviewer can change it.

**Submit button** → `POST /v1/annotations`

**On successful submit:**

A confirmation appears with two options:
1. **"Save to Dataset"** — opens a dropdown of available datasets; on selection calls `POST /v1/annotations/:id/to-dataset-item`; shows success confirmation.
2. **"Next"** — moves to the next pending item in the queue without saving to a dataset.

If the reviewer closes the panel without submitting, no annotation is saved (no draft state in v1).

---

## Rendering Contract

The UI must apply these rules consistently when displaying `input` or `output` values from spans or dataset items:

| Value type | How to render |
|---|---|
| String | Display as-is |
| Object with `messages` array (OpenAI chat format) | Extract the last message with `role: "user"` and display its `content`. Show a "chat format" badge. |
| Object (other) | Display as formatted, syntax-highlighted JSON. Offer "Plain text" toggle. |
| Array | Display as formatted JSON. Offer "Plain text" toggle. |
| `null` or absent | Display as "—" (em dash) |
| Number / boolean | Display as string representation |

In the **Review Queue and Annotation Panel**, the "Plain text" toggle is hidden and JSON is never shown by default. Reviewer always sees extracted readable text.

In the **Developer View** (trace detail, experiment runs), JSON rendering is the default with a toggle to plain text.

---

## Acceptance Criteria

```
GIVEN a project with 3 traces in the last 24 hours, 1 of which has an errored span
WHEN the Dashboard is loaded
THEN the Traces section shows count=3 and error_rate=33%

GIVEN a trace with 4 spans arranged as a tree (root → A → B, root → C)
WHEN the Trace Detail page is loaded
THEN the span tree shows root at top level, A and C as children, B as child of A
AND all spans are expanded by default

GIVEN a span whose output is a JSON object
WHEN the span is selected in Trace Detail
THEN the output is displayed as formatted, syntax-highlighted JSON
AND a "Plain text" toggle is visible

GIVEN a trace not currently in the review queue
WHEN "Add to Queue" is clicked on the Trace Detail page
THEN POST /v1/traces/:id/queue is called
AND the button changes to a disabled "In Queue" state

GIVEN two experiments on the same dataset where experiment B has a higher mean score
WHEN the Comparison view is loaded with A as base and B as compare
THEN the delta is shown in green (positive)
AND rows where B scored lower than A are highlighted red in the per-item table

GIVEN a dataset with 20 items
WHEN the Dataset Detail page is loaded
THEN all items are paginated with 50 per page by default
AND each item's input is truncated to 120 characters with a "Show more" affordance

GIVEN a queue item whose root span input is a JSON object with a messages array
  containing [{"role": "system", "content": "..."}, {"role": "user", "content": "What is the capital of France?"}]
WHEN the queue item is displayed in the Review Queue
THEN the displayed input is "What is the capital of France?"
AND a "chat format" badge is shown

GIVEN a reviewer opens the Annotation Panel and fills in only the Correction field
WHEN Submit is clicked
THEN POST /v1/annotations is called with correction set and label and notes absent
AND the submit succeeds

GIVEN a reviewer opens the Annotation Panel and fills in no fields
WHEN Submit is clicked
THEN the submission is blocked client-side
AND an inline message says "Please provide at least a label, correction, or notes"

GIVEN a successful annotation submission
WHEN the reviewer clicks "Save to Dataset" and selects a dataset
THEN POST /v1/annotations/:id/to-dataset-item is called with the selected dataset_id
AND a success message confirms the item was added

GIVEN a successful annotation submission
WHEN the reviewer clicks "Next"
THEN the panel navigates to the next pending item in the queue
AND no dataset conversion is performed

GIVEN an Experiment Detail page for an experiment with a failed threshold (actual=0.75, threshold=0.80)
WHEN the page loads
THEN a red FAIL badge is visible in the header
AND the gap is displayed as "-0.05"
```

---

## Edge Cases

| Scenario | Expected Behavior |
|---|---|
| Project has no traces | Trace List shows empty state: "No traces yet. Instrument your application to get started." with a link to docs |
| Project has no experiments | Experiment List shows empty state: "No experiments yet. Create a dataset and run your first eval." |
| Review Queue is empty | Queue List shows empty state: "Queue is empty. Add traces to the queue from the Trace Detail view." |
| Trace has no root span (partial trace) | Trace Detail shows a warning banner: "This trace has no root span. Spans may still be ingesting." Root span fields show "—". |
| Span input or output is very large (>10KB) | Display the first 2KB with a "Load full content" button. Do not truncate silently. |
| Experiment comparison requested on two experiments with different datasets | Show error state: "These experiments used different datasets and cannot be compared." with links to each experiment |
| Annotation Panel: reviewer navigates away mid-form | If any field is filled, show a browser-native "Leave page?" confirmation. If all fields are empty, navigate without prompt. |
| Dataset import returns skipped lines | Show a dismissible warning banner: "8 items imported, 2 skipped." with a "View details" toggle showing skipped line numbers and reasons |
| Trace with 0 annotations opened in Annotation Panel via queue | "Previous annotations" section is hidden entirely (not shown as empty) |
